{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch, wandb, platform, gradio, warnings\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of CUDA devices: 1\n",
      "--- CUDA Device 0 ---\n",
      "Name: Quadro GV100\n",
      "Compute Capability: (7, 0)\n",
      "Total Memory: 34077212672 bytes\n",
      "--- CPU Information ---\n",
      "Processor: x86_64\n",
      "System: Linux 6.5.0-28-generic\n",
      "Python Version: 3.12.3\n"
     ]
    }
   ],
   "source": [
    "def print_system_specs():\n",
    "    # Check if CUDA is available\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA Available:\", is_cuda_available)\n",
    "# Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if is_cuda_available:\n",
    "        for i in range(num_cuda_devices):\n",
    "            # Get CUDA device properties\n",
    "            device = torch.device('cuda', i)\n",
    "            print(f\"--- CUDA Device {i} ---\")\n",
    "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
    "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
    "    # Get CPU information\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(\"Processor:\", platform.processor())\n",
    "    print(\"System:\", platform.system(), platform.release())\n",
    "    print(\"Python Version:\", platform.python_version())\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre trained model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"vicgalle/alpaca-gpt4\"\n",
    "\n",
    "# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n",
    "new_model = \"https://huggingface.co/Gnana1306/finetune-llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d65d8b7f11042209be1a602a95ca438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train[0:10000]\")\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c626fa040dd04d4f8cb4536ed70bd9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base model(llama-2-7b-hf) and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgnanaprakash\u001b[0m (\u001b[33mspacefun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gprakash/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gprakash/llm-rag/wandb/run-20240522_105816-r2mfx9oi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B/runs/r2mfx9oi' target=\"_blank\">rural-frog-5</a></strong> to <a href='https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B' target=\"_blank\">https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B/runs/r2mfx9oi' target=\"_blank\">https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B/runs/r2mfx9oi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#monitering login\n",
    "wandb.login(key=\"360e7b904b24ce0619c34c8a5c0a1a34ea6816a9\")\n",
    "run = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha= 8,\n",
    "    lora_dropout= 0.1,\n",
    "    r= 16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir= \"./results\",\n",
    "    num_train_epochs= 1,\n",
    "    per_device_train_batch_size= 8,\n",
    "    gradient_accumulation_steps= 2,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps= 1000,\n",
    "    logging_steps= 30,\n",
    "    learning_rate= 2e-4,\n",
    "    weight_decay= 0.001,\n",
    "    fp16= False,\n",
    "    bf16= False,\n",
    "    max_grad_norm= 0.3,\n",
    "    max_steps= -1,\n",
    "    warmup_ratio= 0.3,\n",
    "    group_by_length= True,\n",
    "    lr_scheduler_type= \"linear\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gprakash/miniconda3/envs/llama2-llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/gprakash/miniconda3/envs/llama2-llm/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/gprakash/miniconda3/envs/llama2-llm/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 2:45:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.776200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.723700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.8447942657470703, metrics={'train_runtime': 9981.5797, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.063, 'total_flos': 8.87101983719424e+16, 'train_loss': 0.8447942657470703, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8784ab7f034593b23dea327128cb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='123.061 MB of 123.082 MB uploaded\\r'), FloatProgress(value=0.9998245947944495, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▆▃▄▂█▂▁▄▂▆▃▁▃▂▄▃▁▃▂▅</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▆▇██▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/loss</td><td>██▃▂▂▃▁▂▂▁▃▁▂▂▁▃▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>8.87101983719424e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>625</td></tr><tr><td>train/grad_norm</td><td>0.18722</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.7004</td></tr><tr><td>train_loss</td><td>0.84479</td></tr><tr><td>train_runtime</td><td>9981.5797</td></tr><tr><td>train_samples_per_second</td><td>1.002</td></tr><tr><td>train_steps_per_second</td><td>0.063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-frog-5</strong> at: <a href='https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B/runs/r2mfx9oi' target=\"_blank\">https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B/runs/r2mfx9oi</a><br/> View project at: <a href='https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B' target=\"_blank\">https://wandb.ai/spacefun/Fine%20tuning%20llama-2-7B</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240522_105816-r2mfx9oi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "wandb.finish()\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "февраль 2021\n",
      "\n",
      "### 1\n",
      "\n",
      "[![](https://img.shields.io/badge/GitHub-100000.svg)](https://github.com/hypersphere-io/hypersphere-c#readme)\n",
      "[![](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/hypersphere-io/hypersphere-c/blob/master/LICENSE)\n",
      "[![](https://img.shields.io/github/forks/hypersphere-io/hypersphere-c.svg)](https://github.com/hypersphere-io/hypersphere-c/network)\n",
      "[![](https://img.shields.io/github/stars/hypersphere-io/hypersphere-c.svg)](https://github.com/hypersphere-io/hypersphere-c/stargazers)\n",
      "[![](https://img.shields.io/github/issues/hypersphere-io/hypersphere-c.svg)](https://github.com/hypersphere-io/hypersphere-c/issues)\n",
      "[![](https://img.shields.io/github/languages/code-size/hypersphere-io/hypersphere-c.svg)](https://github.com/hypersphere-io/hypersphere-c/blob/master/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/hypersphere-c/h\n"
     ]
    }
   ],
   "source": [
    "stream(\"what is newtons 3rd law and its formula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory footprint\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ed140cde2342c8880b3264c67aa1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.float16,\n",
    "    device_map= {\"\": 0})\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45f194d6786475fb829571559417405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4942d84c63443b191164eeba9b4bcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94c964ca5ba45c3a55b78719ffb6e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a83c562c478472f8a78eb470ad236dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc60ae0d82bf492a97a9b47ef5d11d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb25d07e762445eb1a6cf5b9d3b1a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Gnana1306/finetune-llama2/commit/3dd64c5919e9de3f4f40751b8af48c4c58aa2711', commit_message='Upload tokenizer', commit_description='', oid='3dd64c5919e9de3f4f40751b8af48c4c58aa2711', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"finetune-llama2\")\n",
    "tokenizer.push_to_hub(\"finetune-llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref article:\n",
    "1. https://medium.com/@csakash03/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-cf7bb367e790"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
